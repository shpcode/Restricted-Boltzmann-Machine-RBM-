{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shpcode/Restricted-Boltzmann-Machine-RBM-/blob/main/RBM_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SulsHC4il1cQ"
      },
      "source": [
        "# Restricted Boltzmann Machine (RBM) Implementation with Gibbs Sampling\n",
        "\n",
        "This notebook presents an implementation of the Restricted Boltzmann Machine (RBM) with Gibbs sampling, a member of the generative Energy-based model family. RBM utilizes an energy function to compute a score for a given configuration of visible units. The bipartite interaction between visible and hidden captures the high-order interactions among visible nodes.\n",
        "\n",
        "A key advantage of the RBM structure is the independence of each unit in the visible (hidden) layer given the state of the hidden (visible) layer. This enable us to use a rapid sampling scheme known as Gibbs sampling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8KeyMWsl1cO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torchvision.transforms import ToTensor\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr4kHstHl1cT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvZgWeIVl1cT"
      },
      "outputs": [],
      "source": [
        "class RBM:\n",
        "    def __init__(self, n_visible, n_hidden):\n",
        "        # Initializing hyperparameters\n",
        "        self.n = n_visible\n",
        "        self.m = n_hidden\n",
        "        self.h_all = self.combi()  # Generating all possible binary combinations for hidden units\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.softplus = nn.Softplus()\n",
        "    def init(self, betha):\n",
        "        # Initializing model parameters\n",
        "        w = betha * nn.Parameter(torch.nn.init.normal_(torch.empty(self.n, self.m), mean=0.0, std=1))\n",
        "        b = betha * nn.Parameter(torch.nn.init.normal_(torch.empty(1, self.n), mean=0.0, std=0))\n",
        "        c = betha * nn.Parameter(torch.nn.init.normal_(torch.empty(1, self.m), mean=0.0, std=1))\n",
        "        return w, b, c\n",
        "    def combi(self):\n",
        "        # Generating all possible binary combinations for hidden units\n",
        "        lst = list(itertools.product([0, 1], repeat=self.m))\n",
        "        return torch.tensor(lst).float()\n",
        "    def prob_h_given_v(self, visible_states, w, c):\n",
        "        # Computing probability of hidden units given visible units (p(h|V))\n",
        "        return self.sig(torch.matmul(visible_states, w) + c)\n",
        "    def prob_v_given_h(self, hidden_states, w, b):\n",
        "        # Computing probability of visible units given hidden units (p(v|h))\n",
        "        return self.sig(torch.matmul(hidden_states, w.transpose(1, 0)) + b)\n",
        "\n",
        "    # Contrastive Divergence update\n",
        "    def cd_update(self, lr, cd_k, visible_input, w, b, c):\n",
        "        batch_size = torch.tensor(visible_input.shape[0], dtype=float)\n",
        "\n",
        "        # Positive phase\n",
        "        h_probs = self.prob_h_given_v(visible_input, w, c)\n",
        "        h_0 = torch.bernoulli(h_probs)\n",
        "        positive_grads = torch.matmul(torch.transpose(visible_input, 2, 1), h_0)\n",
        "        h_states = h_0\n",
        "\n",
        "        # CD iterations\n",
        "        for step in range(cd_k):\n",
        "            v_probs = self.prob_v_given_h(h_states, w, b)\n",
        "            v_states = torch.bernoulli(v_probs)\n",
        "            h_probs = self.prob_h_given_v(v_states, w, c)\n",
        "            h_states = torch.bernoulli(h_probs)\n",
        "        negative_grads = torch.matmul(torch.transpose(v_states, 2, 1), h_states)\n",
        "\n",
        "        # Update model parameters\n",
        "        w = w + lr * (positive_grads - negative_grads).sum(0)\n",
        "        b = b + lr * (visible_input - v_states).sum(0)\n",
        "        c = c + lr * (h_0 - h_states).sum(0)\n",
        "        return w, b, c\n",
        "\n",
        "    # Free energy calculation\n",
        "    def free_energy(self, input, w, b, c):\n",
        "        v_bias = torch.matmul(input, torch.t(b)).squeeze()\n",
        "        alpha = torch.matmul(input, w).squeeze() + torch.repeat_interleave(c, input.shape[0], 0)\n",
        "        return -(self.softplus(alpha).sum(1) + v_bias).mean()\n",
        "\n",
        "    # Log partition function calculation\n",
        "    def log_z(self, w, b, c):\n",
        "        exponent = []\n",
        "        for i in range(self.h_all.shape[0]):\n",
        "            h_bias = torch.matmul(self.h_all[i, :], torch.t(c)).squeeze()\n",
        "            alpha = (torch.matmul(w, self.h_all[i, :]) + b)\n",
        "            exponent.append([h_bias + self.softplus(alpha).sum()])\n",
        "        expo = torch.tensor(exponent)\n",
        "        expo_m = expo.max()\n",
        "        return torch.log(torch.exp(expo - expo_m).sum()) + expo_m\n",
        "\n",
        "    # Reconstruct visible units from hidden units\n",
        "    def reconstruct(self, input, w, b, c):\n",
        "        hidden_probs = self.prob_h_given_v(input, w, c)\n",
        "        hidden_states = torch.bernoulli(hidden_probs)\n",
        "        visible_probs = self.prob_v_given_h(hidden_states, w, b)\n",
        "        return visible_probs\n",
        "\n",
        "    # Reconstruction error calculation\n",
        "    def reconstruct_error(self, input, w, b, c):\n",
        "        hidden_probs = self.prob_h_given_v(input, w, c)\n",
        "        hidden_states = torch.bernoulli(hidden_probs)\n",
        "        visible_probs = self.prob_v_given_h(hidden_states, w, b)\n",
        "        return torch.mean(torch.square(input - visible_probs)).detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage example\n"
      ],
      "metadata": {
        "id": "6v6fE72Djps6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we use MNIST as toy dataset as an simple example\n",
        "\n",
        "IMAGE_SIZE = 28\n",
        "\n",
        "composed = transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                       lambda x: x>0,\n",
        "                       lambda x: x.float(),\n",
        "            ])\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)\n",
        "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffel= True, batch_size=50)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)\n",
        "\n",
        "\n",
        "valid , bla = next(iter(validation_loader))\n",
        "valid  = torch.flatten(valid, start_dim=2, end_dim=3)"
      ],
      "metadata": {
        "id": "BGB_T2itjrRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUcNBO_bl1cZ"
      },
      "source": [
        "Initializing RBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jntSWXRql1cZ"
      },
      "outputs": [],
      "source": [
        "betha = 0.001 # STD of the Gussian distribtion that generatares intialize the network\n",
        "\n",
        "rbm = RBM(n_visible=IMAGE_SIZE**2, n_hidden=16)\n",
        "w, b, c = rbm.init(betha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3iALJ7al1ca"
      },
      "source": [
        "Training RBM using Contrastive Divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUXMWzhpl1ca"
      },
      "outputs": [],
      "source": [
        "lr = 0.01\n",
        "LL = []\n",
        "cd_k = 8 # To study the effect of cd steps, we can add a loop on the value of cd_k\n",
        "error = []\n",
        "epochs = 10\n",
        "i = -1\n",
        "for epoch in range(epochs):\n",
        "    for x, y in train_loader:\n",
        "        input = torch.flatten(x, start_dim=2, end_dim=3)\n",
        "        i = i + 1\n",
        "        if i == 200 or i % 500 == 0:\n",
        "            LL.append([cd_k, -rbm.free_energy(valid, w, b, c) - rbm.log_z(w, b, c)])\n",
        "        w, b, c = rbm.cd_update(lr / (2 * epoch + 1), cd_k, input, w, b, c)\n",
        "        b = torch.zeros(b.shape)\n",
        "\n",
        "ll = np.asarray(LL)\n",
        "# np.save('shuffel-neg-ll', ll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EJrWPRHl1cc"
      },
      "source": [
        "Generating new samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQgreHxgl1cc"
      },
      "outputs": [],
      "source": [
        "x = 1 * torch.flatten(validation_dataset[25][0]) + torch.normal(torch.zeros(IMAGE_SIZE**2), std=0.2)\n",
        "f, axarr = plt.subplots(1, 7)\n",
        "f.set_figheight(15)\n",
        "f.set_figwidth(15)\n",
        "axarr[0].imshow(x.reshape(IMAGE_SIZE, IMAGE_SIZE))\n",
        "j = 0\n",
        "for i in range(120):\n",
        "    x = rbm.reconstruct(x, w, b, c)\n",
        "    if i % 20 == 0:\n",
        "        j = j + 1\n",
        "        im = x.detach().numpy()\n",
        "        axarr[j].imshow(im.reshape(IMAGE_SIZE, IMAGE_SIZE))\n",
        "        axarr[j].set_title('%i gibbs sampled' % i)\n",
        "plt.savefig('num-generat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV_UPTJXl1cc"
      },
      "source": [
        "Plotting negative log-likelihood progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf6aUhEcl1cd"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.asarray(LL)[:, 1])\n",
        "plt.ylabel('neg-ll')\n",
        "plt.xlabel('updates')\n",
        "plt.savefig('nn-ll')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXuKtdhyl1ce"
      },
      "source": [
        "Generating samples"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}